{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We need to access OpenAI API. I use the older version, but there is a new one (1.xx), which is functionally equivalent for these purposes but with a different syntax. Feel free to modify the prompt function if you want to use the latest openai package."
      ],
      "metadata": {
        "id": "tw9S1uMhKeCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F9zQvJo6DDqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fdedda-bb68-4b3d-e703-c207417028a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "id": "R9e8MkMLabca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7611a619-3c3f-4032-96c4-5d68ebc72b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m944.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use pandas to have data in a nice data structure, and time to keep track of created files"
      ],
      "metadata": {
        "id": "ZNtX7mlbK2hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from time import strftime"
      ],
      "metadata": {
        "id": "UmplC-kSaZWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple function to help send and receive data from ChatGPT API. It loops up to 5 times in case OpenAI fails (which happens randomly sometimes). Here you'd also adjust the model parameters, according to the OpenAI API reference (start here: https://platform.openai.com/docs/api-reference/chat)"
      ],
      "metadata": {
        "id": "7LDt6ltVLCmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt(Q):\n",
        "  tries = 0\n",
        "  while tries<5:\n",
        "    try:\n",
        "      response = openai.ChatCompletion.create(\n",
        "        model=mmodel,\n",
        "        messages=Q,\n",
        "        temperature=0.5,\n",
        "        #max_tokens=tkn,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        request_timeout=199\n",
        "      )['choices'][0]['message']['content']\n",
        "      break\n",
        "    except Exception as e:\n",
        "      tries = tries+1\n",
        "      print(f\"An error occurred {tries:d}th time: {e}\")\n",
        "  return(Q,response)"
      ],
      "metadata": {
        "id": "ow0RRmv-c0g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get time and date for convenient file naming"
      ],
      "metadata": {
        "id": "2hhwjQWdLWZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtime = strftime(\"%Y_%m_%d-%H%M%S\")"
      ],
      "metadata": {
        "id": "8a1ayoovdXxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you choose the model to use and provide the API key\n"
      ],
      "metadata": {
        "id": "FMGnirCec-yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mmodel = \"gpt-3.5-turbo-1106\"\n",
        "openai.api_key = \"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\""
      ],
      "metadata": {
        "id": "L12bDXihamIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have the prompts we will be using, as well as the system prompt which servers as a general global set of instructions or information. The system prompt may be left empty.\n",
        "\n",
        "I have two prompts, one asks to generate a table, and the second one iteratively asks to continue that table. This is likely not an optimal approach, but it gives some results so a good example.\n",
        "\n",
        "I also separated the property from the prompt so that I can vary the property and keep the same prompts, and the other way around. It does not have to be that way if more task specific prompts work better."
      ],
      "metadata": {
        "id": "YX_8daefLe7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROPERTY = 'yield strength'\n",
        "\n",
        "system = 'You follow instructions carefully, you provide as many rows of the table as possible, at least 50 for every prompt.'\n",
        "\n",
        "# 2024/2/19: abbrev -> full name\n",
        "qs = [\"Provide me with a list of \"+PROPERTY+\" values for different materials. Your response should be a table consisting of 2 columns: material, value. The materials have to be typed as unique chemical compositions consisting of chemical element abbreviations and numbers only (e.g. GaAs, but not Gallium Arsenide). The values have to be single numbers, not ranges. With unit after the numbers. Type out as many different values as you can. Include only High entropy alloys. You are not allowed to type anything else than this table.\",\n",
        "      \"Continue expanding this table with new values of \"+PROPERTY+\" making sure you do not duplicate entries. Type out as many different values as you can. You are not allowed to type anything else than this table.\"]\n"
      ],
      "metadata": {
        "id": "EoRSyBE1a3WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the file a uniquely identifiable name. We want to save EVERYTHING or we will lose it, and each generation costs money. Either link up your google drive, or download all files each time because they are temporary (if executed on colab)."
      ],
      "metadata": {
        "id": "IGM345OOOK0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = PROPERTY.replace(\" \", \"\")+'_'+dtime+'.csv'\n",
        "print(f\"Saving to: {filename} and {filename.replace('csv','txt')}\")"
      ],
      "metadata": {
        "id": "bzAsK-xAOKTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665ba115-78a7-4d17-b1d2-ec5186a198f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to: yieldstrength_2024_03_17-165957.csv and yieldstrength_2024_03_17-165957.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set up a structure for the chat. I like to hold the conversation in a list, and then each prompt and response inside it are dictionaries, as per OpenAI requirements. I start with just the system prompt and will append to that later.\n",
        "\n",
        "I also set up some empty lists and initial values to keep track of progress"
      ],
      "metadata": {
        "id": "YcorW2LDOxGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sss = [{\"role\": \"system\", \"content\": system}]\n",
        "tab = []\n",
        "tab_clean = []\n",
        "ur = 0\n",
        "um = 0\n",
        "i = 0"
      ],
      "metadata": {
        "id": "vkrmS0FMOtgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main loop that will loop over the prompts, get responses and try to put them in a structured data format.\n",
        "\n",
        "This while approach is VERY SIMPLE and does not account for many things that may be the response deom the model, so it is only an example to build upon based on the result, not a solution ready to be used. It does not even have to be used at all if one has a better/different idea."
      ],
      "metadata": {
        "id": "Br38sWQCPKfn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNMePOTgZ8pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57010f3e-efff-454a-d4a2-0c72be735cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:   1 Generated_rows: 266;     TOTAL:  Uniq_rows:    1   Uniq_materials:    1\n",
            "Stopping due to NO PROGRESS\n",
            "           Material  Value\n",
            "265   CoCrFeMnNi89     488\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/drive/Shareddrives/LLM_Project/\"\n",
        "\n",
        "while True:\n",
        "  # here i send my first prompt first and then loop over the second one over and\n",
        "  # over again. You will likely have a different approach to this.\n",
        "  if i<1:\n",
        "    qq = qs[0]\n",
        "  else:\n",
        "    qq = qs[1]\n",
        "  # save the first prompt to the conversation\n",
        "  sss.append({\"role\": \"user\", \"content\": qq})\n",
        "  # send out the first prompt and receive the response\n",
        "  sss,ans = prompt(sss)\n",
        "  # save the response to the conversation\n",
        "  sss.append({\"role\": \"assistant\", \"content\": ans})\n",
        "\n",
        "  # we are saving the raw prompts and raw responses, in case we want to analyze\n",
        "  # or postprocess later\n",
        "  with open(path+filename.replace('csv','txt'), 'a') as file:\n",
        "    print(\"USER: \"+qq, file=file)\n",
        "    print(\"GPT : \"+ans, file=file)\n",
        "\n",
        "  # Here we start to grab the data into a nicer structure. For simplicity I had\n",
        "  # a lot of assumptions. I assume that the word 'value' exists in the header\n",
        "  # (see first prompt), I remove the header, the separator, and get the rest.\n",
        "  lines = ans.split('\\n')\n",
        "  if 'value' in lines[0].lower():\n",
        "    ans = '\\n'.join(lines[1:])\n",
        "  lines = ans.split('\\n')\n",
        "  if '----' in lines[0].lower():\n",
        "    ans = '\\n'.join(lines[1:])\n",
        "  lines = ans.split('\\n')\n",
        "\n",
        "  # here I try to split the table by columns. I'm assuming that they are\n",
        "  # separated with |, which is not necessary always the case.\n",
        "  tab.append(ans)\n",
        "  try:\n",
        "    for line in tab[-1].strip().split('\\n'):\n",
        "      tab_clean.append(line.strip('|').split('|'))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # another assumption - only two columns, \"material\" and \"value\" (see prompt)\n",
        "  # some cleanup, converting strings to numbers etc.\n",
        "  # there is no error handling or edge cases, for example 1.6-1.7 will be removed\n",
        "  # because it technically is not a number (although it kind of is).\n",
        "  df = pd.DataFrame(tab_clean, columns=['Material', 'Value'])\n",
        "  df = df[pd.to_numeric(df['Value'], errors='coerce').notna()]\n",
        "  df['Value'] = pd.to_numeric(df['Value'])\n",
        "  df.to_csv(path+filename, index=False)\n",
        "\n",
        "  # here I count how many new (non-duplicate) materials we are extracting each\n",
        "  # time, to monitor progress. We stop if more than 10 iterations or not progress\n",
        "  if len(df.drop_duplicates()) > ur or df['Material'].nunique() > um:\n",
        "    ur = len(df.drop_duplicates())\n",
        "    um = df['Material'].nunique()\n",
        "    i = i+1\n",
        "    if i > 10:\n",
        "      print(\"Stopping due to 10 iterations exceeded\")\n",
        "      break\n",
        "  else:\n",
        "    print(\"Stopping due to NO PROGRESS\")\n",
        "    break\n",
        "\n",
        "  print(f\"Iteration: {i:3} Generated_rows: {len(lines):3};     TOTAL:  Uniq_rows: {len(df.drop_duplicates()):4d}   Uniq_materials: {df['Material'].nunique():4d}\")\n",
        "\n",
        "print(df)"
      ]
    }
  ]
}